# üìã SPRINT 24 - PLANNING & ARCHITECTURE
## Server-Sent Events (SSE) Streaming Implementation

**Data**: November 14, 2025, 08:45 -03:00  
**Sprint**: 24 - Streaming SSE  
**Status**: üîÑ PLANEJAMENTO COMPLETO

---

## üéØ SPRINT GOAL

**Implementar streaming de respostas via Server-Sent Events (SSE)** para eliminar depend√™ncia de timeout √∫nico e alcan√ßar taxa de sucesso de **90%+** (vs 25% atual).

---

## üìä CONTEXTO & JUSTIFICATIVA

### Problema Identificado (Sprint 23)
- ‚úÖ Timeout aumentado: 30s ‚Üí 120s ‚Üí 300s
- ‚ùå Taxa de sucesso: Mantida em ~25%
- üîç **Descoberta**: Prompts complexos precisam >300s
- üí° **Conclus√£o**: Timeout n√£o resolve, streaming sim!

### Por que Streaming?
1. **Elimina timeout √∫nico**: N√£o h√° limite de tempo total
2. **UX superior**: Usu√°rio v√™ progresso em tempo real
3. **Padr√£o ind√∫stria**: ChatGPT, Claude, Copilot usam
4. **Taxa de sucesso esperada**: 90%+ (vs 25% atual)

---

## üèóÔ∏è ARQUITETURA DA SOLU√á√ÉO

### Fluxo de Dados Atual (Problem√°tico)
```
Frontend ‚Üí Backend ‚Üí LM Studio
   ‚Üì          ‚Üì          ‚Üì
 WAIT      WAIT      Process (>300s)
   ‚Üì          ‚Üì          ‚Üì
 WAIT      WAIT      Response
   ‚Üì          ‚Üì          ‚Üì
Response ‚Üê Response ‚Üê Complete
   ‚Üì
Timeout ‚ùå (se >300s)
```

### Fluxo de Dados Novo (Streaming)
```
Frontend ‚Üí Backend ‚Üí LM Studio
   ‚Üì          ‚Üì          ‚Üì
EventSource SSE     stream: true
   ‚Üì          ‚Üì          ‚Üì
Chunk 1  ‚Üê Chunk 1 ‚Üê Token batch
Chunk 2  ‚Üê Chunk 2 ‚Üê Token batch
Chunk 3  ‚Üê Chunk 3 ‚Üê Token batch
   ...        ...        ...
Complete ‚Üê Complete ‚Üê Done
   ‚Üì
NEVER timeout! ‚úÖ
```

---

## üõ†Ô∏è COMPONENTES A IMPLEMENTAR

### 1. Backend - LM Studio Client (server/lib/lm-studio.ts)

#### Novo M√©todo: `chatCompletionStream()`
```typescript
async *chatCompletionStream(
  request: LMStudioRequest
): AsyncGenerator<string, void, unknown> {
  const response = await fetch(`${this.baseUrl}/v1/chat/completions`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      ...request,
      stream: true,  // KEY: Enable streaming
    }),
  });

  const reader = response.body!.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    const lines = chunk.split('\n').filter(line => line.trim());

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6);
        if (data === '[DONE]') return;
        
        try {
          const parsed = JSON.parse(data);
          const content = parsed.choices[0]?.delta?.content;
          if (content) yield content;
        } catch (e) {
          // Skip invalid JSON
        }
      }
    }
  }
}
```

### 2. Backend - REST API Endpoint (server/routes/rest-api.ts)

#### Novo Endpoint: `POST /api/prompts/execute/stream`
```typescript
// SSE streaming endpoint
app.post('/api/prompts/execute/stream', async (req, res) => {
  try {
    const { promptId, variables } = req.body;

    // Set SSE headers
    res.setHeader('Content-Type', 'text/event-stream');
    res.setHeader('Cache-Control', 'no-cache');
    res.setHeader('Connection', 'keep-alive');

    // Fetch prompt and model
    const [prompt] = await db.select()
      .from(prompts)
      .where(eq(prompts.id, promptId));

    const [model] = await db.select()
      .from(aiModels)
      .where(eq(aiModels.id, prompt.modelId));

    // Process content
    let content = prompt.content;
    if (variables) {
      Object.entries(variables).forEach(([key, value]) => {
        content = content.replace(
          new RegExp(`{{${key}}}`, 'g'),
          String(value)
        );
      });
    }

    // Send initial metadata
    res.write(`data: ${JSON.stringify({
      type: 'start',
      promptId,
      modelId: model.id,
      modelName: model.name
    })}\n\n`);

    // Stream from LM Studio
    for await (const chunk of lmStudio.chatCompletionStream({
      model: model.modelId,
      messages: [{ role: 'user', content }],
      temperature: prompt.temperature,
      max_tokens: prompt.maxTokens,
    })) {
      res.write(`data: ${JSON.stringify({
        type: 'chunk',
        content: chunk
      })}\n\n`);
    }

    // Send completion
    res.write(`data: ${JSON.stringify({
      type: 'done'
    })}\n\n`);

    res.end();
  } catch (error) {
    res.write(`data: ${JSON.stringify({
      type: 'error',
      message: error.message
    })}\n\n`);
    res.end();
  }
});
```

### 3. Frontend - Hook React (client/src/hooks/useStreamingPrompt.ts)

#### Novo Hook: `useStreamingPrompt()`
```typescript
import { useState, useCallback } from 'react';

interface StreamingState {
  content: string;
  isStreaming: boolean;
  error: string | null;
  metadata: any;
}

export function useStreamingPrompt() {
  const [state, setState] = useState<StreamingState>({
    content: '',
    isStreaming: false,
    error: null,
    metadata: null,
  });

  const execute = useCallback(async (promptId: number, variables?: any) => {
    setState({
      content: '',
      isStreaming: true,
      error: null,
      metadata: null,
    });

    try {
      const eventSource = new EventSource(
        `/api/prompts/execute/stream?${new URLSearchParams({
          promptId: String(promptId),
          variables: JSON.stringify(variables || {}),
        })}`
      );

      eventSource.onmessage = (event) => {
        const data = JSON.parse(event.data);

        switch (data.type) {
          case 'start':
            setState(prev => ({
              ...prev,
              metadata: data,
            }));
            break;

          case 'chunk':
            setState(prev => ({
              ...prev,
              content: prev.content + data.content,
            }));
            break;

          case 'done':
            setState(prev => ({
              ...prev,
              isStreaming: false,
            }));
            eventSource.close();
            break;

          case 'error':
            setState(prev => ({
              ...prev,
              error: data.message,
              isStreaming: false,
            }));
            eventSource.close();
            break;
        }
      };

      eventSource.onerror = () => {
        setState(prev => ({
          ...prev,
          error: 'Connection error',
          isStreaming: false,
        }));
        eventSource.close();
      };
    } catch (error) {
      setState(prev => ({
        ...prev,
        error: error.message,
        isStreaming: false,
      }));
    }
  }, []);

  return { ...state, execute };
}
```

### 4. Frontend - UI Component (client/src/components/StreamingPromptExecutor.tsx)

#### Novo Component: `<StreamingPromptExecutor />`
```typescript
import React, { useState } from 'react';
import { useStreamingPrompt } from '../hooks/useStreamingPrompt';

export function StreamingPromptExecutor({ promptId }: { promptId: number }) {
  const { content, isStreaming, error, metadata, execute } = useStreamingPrompt();
  const [variables, setVariables] = useState({});

  return (
    <div className="streaming-executor">
      {/* Input for variables */}
      <div className="variables-input">
        <textarea
          placeholder="Variables (JSON)"
          onChange={(e) => setVariables(JSON.parse(e.target.value || '{}'))}
        />
      </div>

      {/* Execute button */}
      <button
        onClick={() => execute(promptId, variables)}
        disabled={isStreaming}
      >
        {isStreaming ? 'Streaming...' : 'Execute Prompt'}
      </button>

      {/* Progress indicator */}
      {isStreaming && (
        <div className="streaming-indicator">
          <div className="spinner" />
          <span>Receiving response...</span>
        </div>
      )}

      {/* Metadata */}
      {metadata && (
        <div className="metadata">
          <span>Model: {metadata.modelName}</span>
        </div>
      )}

      {/* Streaming content */}
      <div className="content-area">
        <pre>{content}</pre>
        {isStreaming && <span className="cursor">‚ñä</span>}
      </div>

      {/* Error display */}
      {error && (
        <div className="error">
          Error: {error}
        </div>
      )}
    </div>
  );
}
```

---

## üìã SPRINT BACKLOG (15 TASKS)

### Fase 1: Planejamento & An√°lise (Tasks 24.1-24.3)
- [x] **24.1** - Planejamento: Criar Sprint 24 backlog
- [ ] **24.2** - An√°lise: Estudar SSE no LM Studio
- [ ] **24.3** - Design: Arquitetar solu√ß√£o completa

### Fase 2: Backend Implementation (Tasks 24.4-24.6)
- [ ] **24.4** - Backend: Modificar LMStudioClient
  - Adicionar m√©todo `chatCompletionStream()`
  - Implementar AsyncGenerator
  - Parsear SSE do LM Studio
- [ ] **24.5** - Backend: Atualizar REST API
  - Criar endpoint `/api/prompts/execute/stream`
  - Configurar headers SSE
  - Implementar streaming pipeline
- [ ] **24.6** - Backend: Middleware streaming
  - Error handling para conex√µes perdidas
  - Cleanup de recursos
  - Logging de eventos

### Fase 3: Frontend Implementation (Tasks 24.7-24.9)
- [ ] **24.7** - Frontend: Hook useStreamingPrompt
  - Estado de streaming
  - EventSource connection
  - Acumula√ß√£o de chunks
- [ ] **24.8** - Frontend: UI atualizada
  - Componente StreamingPromptExecutor
  - √Årea de conte√∫do progressivo
  - Integra√ß√£o com p√°ginas existentes
- [ ] **24.9** - Frontend: Indicador visual
  - Spinner/loading animation
  - Progress bar
  - Cursor piscante

### Fase 4: Build & Deploy (Tasks 24.10-24.11)
- [ ] **24.10** - Build completo
  - `npm run build` (backend + frontend)
  - Verificar bundle size
  - Testar localmente
- [ ] **24.11** - Deploy produ√ß√£o
  - SCP para servidor
  - Rebuild no servidor
  - PM2 restart
  - Verificar status

### Fase 5: Testing & Validation (Tasks 24.12-24.14)
- [ ] **24.12** - Teste 1: Prompt simples
  - Executar prompt r√°pido (<30s)
  - Verificar streaming funciona
  - Confirmar chunks recebidos
- [ ] **24.13** - Teste 2: Prompt complexo
  - Executar prompt >300s
  - Verificar N√ÉO timeout
  - Confirmar resposta completa
- [ ] **24.14** - Teste 3: M√∫ltiplas simult√¢neas
  - 3 requests paralelas
  - Verificar estabilidade
  - Confirmar n√£o interferem

### Fase 6: Documentation (Task 24.15)
- [ ] **24.15** - Documenta√ß√£o completa
  - Commit com mensagem detalhada
  - Push para GitHub
  - Sprint 24 Final Report
  - Sync servidor

---

## üéØ DEFINITION OF DONE

### T√©cnico
- [ ] C√≥digo implementado e funcionando
- [ ] LMStudioClient suporta streaming
- [ ] Endpoint SSE funcionando
- [ ] Frontend recebe chunks progressivos
- [ ] Build sem erros
- [ ] Deploy completo

### Funcional
- [ ] Prompt simples completa com streaming
- [ ] Prompt complexo (>300s) completa SEM timeout
- [ ] M√∫ltiplas requisi√ß√µes n√£o interferem
- [ ] Taxa de sucesso >75% (vs 25% atual)
- [ ] UX melhorada (feedback progressivo)

### Qualidade
- [ ] Testes executados (3 baterias)
- [ ] Error handling robusto
- [ ] Logs apropriados
- [ ] Performance adequada
- [ ] Sem memory leaks

### Documenta√ß√£o
- [ ] C√≥digo comentado
- [ ] Sprint report completo (SCRUM + PDCA)
- [ ] Commit messages claras
- [ ] PR criado e merged
- [ ] README atualizado se necess√°rio

---

## üìä M√âTRICAS DE SUCESSO

### Quantitativas
| M√©trica | Antes | Meta | Como Medir |
|---------|-------|------|------------|
| Taxa de sucesso | 25% | >75% | Testes 1-3 |
| Timeout errors | 75% | <10% | Logs PM2 |
| Time to first byte | N/A | <2s | Frontend logs |
| Total completion | 300s+ | Any | Sem limite |

### Qualitativas
- ‚úÖ Usu√°rio v√™ progresso em tempo real
- ‚úÖ N√£o h√° "espera cega"
- ‚úÖ Sistema n√£o trava em prompts longos
- ‚úÖ UX compar√°vel a ChatGPT/Claude

---

## üîÑ PDCA CYCLE - SPRINT 24

### PLAN (Ë®àÁîª - Keikaku)
**Problema**: 75% prompts timeoutam em 300s  
**Hip√≥tese**: Streaming elimina depend√™ncia de timeout  
**Solu√ß√£o**: Implementar SSE (Server-Sent Events)  
**Meta**: Taxa de sucesso >75%

### DO (ÂÆüË°å - Jikk≈ç)
**Implementar**:
1. Backend streaming (LMStudioClient + REST API)
2. Frontend EventSource (hook + UI)
3. Deploy completo
4. Testes abrangentes

### CHECK (Ë©ï‰æ° - Hy≈çka)
**Validar**:
1. Prompts simples funcionam
2. Prompts complexos N√ÉO timeoutam
3. M√∫ltiplas requests est√°veis
4. Taxa de sucesso >75%

### ACT (ÊîπÂñÑ - Kaizen)
**Aprender**:
1. Se funcionar: Padr√£o para todas APIs
2. Se problemas: Ajustes e itera√ß√£o
3. Documentar li√ß√µes aprendidas

---

## ‚ö†Ô∏è RISCOS & MITIGA√á√ïES

### Risco 1: LM Studio n√£o suporta streaming
**Probabilidade**: Baixa  
**Impacto**: Alto  
**Mitiga√ß√£o**: Validar API docs primeiro (Task 24.2)

### Risco 2: EventSource n√£o funciona em produ√ß√£o
**Probabilidade**: M√©dia  
**Impacto**: Alto  
**Mitiga√ß√£o**: Testar localmente antes de deploy

### Risco 3: Performance degradada
**Probabilidade**: Baixa  
**Impacto**: M√©dio  
**Mitiga√ß√£o**: Monitorar CPU/memory durante testes

### Risco 4: Frontend crashes com chunks grandes
**Probabilidade**: M√©dia  
**Impacto**: M√©dio  
**Mitiga√ß√£o**: Buffer chunks e update em batch

---

## üöÄ PR√ìXIMOS PASSOS IMEDIATOS

1. ‚úÖ **Task 24.1**: Planejamento completo ‚Üí **CONCLU√çDO**
2. ‚è≥ **Task 24.2**: An√°lise SSE no LM Studio
3. ‚è≥ **Task 24.3**: Design detalhado da arquitetura
4. ‚è≥ **Task 24.4**: Come√ßar implementa√ß√£o backend

---

## üìö REFER√äNCIAS T√âCNICAS

### LM Studio API
- Endpoint: `http://localhost:1234/v1/chat/completions`
- Param: `stream: true` para SSE
- Format: `data: {...}\n\n`

### Server-Sent Events (SSE)
- Protocol: HTTP
- Content-Type: `text/event-stream`
- Format: `data: json\n\n`
- Browser API: `EventSource`

### Exemplos Ind√∫stria
- **OpenAI**: Usa SSE para streaming
- **Anthropic Claude**: Usa SSE
- **GitHub Copilot**: Usa streaming similar

---

**Preparado Por**: GenSpark AI Developer  
**Data**: November 14, 2025, 08:50 -03:00  
**Sprint**: 24  
**Status**: üîÑ PLANEJAMENTO COMPLETO ‚Üí INICIANDO IMPLEMENTA√á√ÉO  
**Progress**: 1/15 tasks (6.7%)

---

## ‚úÖ TASK 24.1 COMPLETA

Planejamento detalhado criado com:
- ‚úÖ 15 tasks definidas
- ‚úÖ Arquitetura desenhada
- ‚úÖ Componentes especificados
- ‚úÖ M√©tricas estabelecidas
- ‚úÖ Riscos identificados

**Pr√≥ximo**: Task 24.2 - An√°lise SSE no LM Studio
