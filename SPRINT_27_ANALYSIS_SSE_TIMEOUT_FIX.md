# üìã SPRINT 27 - AN√ÅLISE E PLANEJAMENTO

**T√≠tulo**: Corre√ß√£o de Timeout em Streaming SSE para Prompts Longos  
**Data de Cria√ß√£o**: 15 de novembro de 2025, 00:15 -03:00  
**Metodologia**: SCRUM + PDCA  
**Sprint Origem**: Sprint 26 (Rodada 33)  
**Problema Identificado**: #1 - SSE Timeout em Prompts Longos

---

## üéØ OBJETIVO DA SPRINT

**Meta Principal**: Eliminar timeout em streaming SSE para prompts que geram respostas longas (> 30 segundos)

**Crit√©rios de Sucesso**:
- ‚úÖ Streaming completa sem timeout para prompts de at√© 2000 chunks
- ‚úÖ Frontend recebe evento DONE corretamente
- ‚úÖ Usu√°rio v√™ progresso em tempo real
- ‚úÖ Tempo de resposta configur√°vel por prompt
- ‚úÖ Testes automatizados aprovados (100%)

---

## üìä AN√ÅLISE DO PROBLEMA (PLAN - PDCA)

### Situa√ß√£o Atual

**Problema Detectado na Rodada 33**:
```
Teste: Streaming SSE Endpoint
Status: PARCIALMENTE APROVADO (5/7 checks)
Falhas:
  ‚ùå Received DONE event (timeout ap√≥s 30s)
  ‚ùå Response time < 30s (30.3s real)

Logs do Servidor:
  ‚úÖ Stream completed - 1999 chunks, 55628ms, 5154 chars
  ‚úÖ Evento DONE enviado pelo backend
  ‚ùå Frontend timeout antes de receber DONE
```

### Causa Raiz

**An√°lise dos 5 Porqu√™s**:
1. **Por que o teste falhou?** ‚Üí Timeout de 30s foi atingido
2. **Por que o timeout foi atingido?** ‚Üí Modelo demorou 55s para completar
3. **Por que o modelo demorou tanto?** ‚Üí Prompt gerou 1999 chunks (5154 caracteres)
4. **Por que gerou tantos chunks?** ‚Üí Sem limite de tokens configurado
5. **Por que n√£o h√° limite?** ‚Üí LM Studio request n√£o especifica `max_tokens`

**Causa Raiz**: Requisi√ß√µes ao LM Studio n√£o limitam tamanho da resposta

### Impacto

**Usu√°rio Final**:
- ‚ö†Ô∏è Experi√™ncia ruim com prompts longos
- ‚ö†Ô∏è Interface pode parecer travada (> 30s sem feedback final)
- ‚ö†Ô∏è Sem controle sobre tamanho da resposta

**Sistema**:
- ‚ö†Ô∏è Consumo excessivo de recursos do LM Studio
- ‚ö†Ô∏è Timeout em testes automatizados
- ‚ö†Ô∏è Poss√≠vel timeout em clientes HTTP padr√£o (60s)

---

## üéØ SOLU√á√ÉO PROPOSTA (PLAN - PDCA)

### Estrat√©gia

**Abordagem Multi-Camadas**:

1. **Camada Backend** (LM Studio Client)
   - Adicionar `max_tokens` parameter em requests
   - Configurar default: 1024 tokens (~750 palavras)
   - Permitir override por prompt

2. **Camada Frontend** (useStreamingPrompt)
   - Implementar timeout configur√°vel
   - Default: 120 segundos (2 minutos)
   - Mensagem clara ao usu√°rio em timeout

3. **Camada UI** (StreamingPromptExecutor)
   - Progress bar com tempo estimado
   - Indicador de "tempo restante"
   - Op√ß√£o "Continuar aguardando" se timeout

### Arquitetura da Solu√ß√£o

```typescript
// Backend: server/lib/lm-studio.ts
interface LMStudioRequest {
  model: string;
  messages: Message[];
  stream: boolean;
  max_tokens?: number;      // NEW: Limite de tokens
  temperature?: number;
}

// Frontend: client/src/hooks/useStreamingPrompt.ts
interface ExecuteOptions {
  promptId: number;
  variables?: Record<string, any>;
  modelId: number;
  timeout?: number;          // NEW: Timeout configur√°vel (ms)
  maxTokens?: number;        // NEW: Limite de tokens
}

// Backend: server/routes/rest-api.ts
router.post('/prompts/execute/stream', async (req, res) => {
  const { maxTokens = 1024, timeout = 120000 } = req.body;
  
  // Aplicar timeout no streaming
  const timeoutId = setTimeout(() => {
    res.write(`data: ${JSON.stringify({
      type: 'timeout',
      message: 'Streaming timeout',
      duration: timeout
    })}\n\n`);
    res.end();
  }, timeout);
  
  // Limpar timeout ao completar
  clearTimeout(timeoutId);
});
```

---

## üìù BACKLOG SCRUM - SPRINT 27

### User Stories

**US-27.1**: Como desenvolvedor, quero limitar tokens em LM Studio para evitar respostas excessivamente longas

**Crit√©rios de Aceite**:
- Backend adiciona `max_tokens` em requests ao LM Studio
- Default: 1024 tokens
- Configur√°vel por prompt no banco de dados
- Documentado na API

---

**US-27.2**: Como usu√°rio, quero ver um timeout configur√°vel para que prompts longos n√£o falhem silenciosamente

**Crit√©rios de Aceite**:
- Frontend aceita `timeout` parameter em `execute()`
- Default: 120000ms (2 minutos)
- Mensagem clara ao usu√°rio em caso de timeout
- Op√ß√£o de retry autom√°tico

---

**US-27.3**: Como usu√°rio, quero ver progresso em tempo real para saber quanto falta para completar

**Crit√©rios de Aceite**:
- Progress bar visual com % estimado
- "Tempo estimado restante" baseado em chunks/s
- Indicador de chunks processados (X/Y chunks)

---

### Tarefas T√©cnicas (30 tarefas)

#### FASE 1: Backend - LM Studio Client (8 tarefas)

**T-27.1** - Adicionar `max_tokens` interface em LMStudioRequest  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `server/lib/lm-studio.ts`

**T-27.2** - Implementar `max_tokens` parameter em `chatCompletionStream()`  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `server/lib/lm-studio.ts`

**T-27.3** - Adicionar `temperature` parameter (bonus)  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `server/lib/lm-studio.ts`

**T-27.4** - Criar testes unit√°rios para LM Studio client  
**Complexidade**: 3 pontos | **Tempo**: 45 min  
**Arquivo**: `server/lib/__tests__/lm-studio.test.ts`

**T-27.5** - Adicionar logging de tokens em resposta  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `server/lib/lm-studio.ts`

**T-27.6** - Adicionar valida√ß√£o de `max_tokens` (min: 50, max: 4096)  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `server/lib/lm-studio.ts`

**T-27.7** - Documentar novos parameters na interface  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `server/lib/lm-studio.ts`

**T-27.8** - Build e valida√ß√£o TypeScript  
**Complexidade**: 1 ponto | **Tempo**: 10 min  

---

#### FASE 2: Backend - REST API Endpoint (7 tarefas)

**T-27.9** - Adicionar `maxTokens` e `timeout` em request body schema  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `server/routes/rest-api.ts`

**T-27.10** - Implementar timeout protection com `setTimeout()`  
**Complexidade**: 3 pontos | **Tempo**: 45 min  
**Arquivo**: `server/routes/rest-api.ts`

**T-27.11** - Enviar evento SSE `timeout` ao atingir limite  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `server/routes/rest-api.ts`

**T-27.12** - Limpar `setTimeout` ao completar stream  
**Complexidade**: 2 pontos | **Tempo**: 20 min  
**Arquivo**: `server/routes/rest-api.ts`

**T-27.13** - Passar `max_tokens` para LM Studio client  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `server/routes/rest-api.ts`

**T-27.14** - Adicionar logs detalhados (tokens, duration, chunks)  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `server/routes/rest-api.ts`

**T-27.15** - Criar testes end-to-end do endpoint  
**Complexidade**: 3 pontos | **Tempo**: 60 min  
**Arquivo**: `server/routes/__tests__/rest-api-streaming.test.ts`

---

#### FASE 3: Frontend - useStreamingPrompt Hook (6 tarefas)

**T-27.16** - Adicionar `timeout` e `maxTokens` em ExecuteOptions  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `client/src/hooks/useStreamingPrompt.ts`

**T-27.17** - Implementar timeout client-side com `setTimeout()`  
**Complexidade**: 3 pontos | **Tempo**: 45 min  
**Arquivo**: `client/src/hooks/useStreamingPrompt.ts`

**T-27.18** - Adicionar handler para evento SSE `timeout`  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `client/src/hooks/useStreamingPrompt.ts`

**T-27.19** - Calcular progresso estimado (chunks/s, ETA)  
**Complexidade**: 3 pontos | **Tempo**: 45 min  
**Arquivo**: `client/src/hooks/useStreamingPrompt.ts`

**T-27.20** - Adicionar state `timeoutOccurred` e `timeoutMessage`  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `client/src/hooks/useStreamingPrompt.ts`

**T-27.21** - Atualizar interface TypeScript e JSDoc  
**Complexidade**: 1 ponto | **Tempo**: 15 min  
**Arquivo**: `client/src/hooks/useStreamingPrompt.ts`

---

#### FASE 4: Frontend - StreamingPromptExecutor UI (5 tarefas)

**T-27.22** - Adicionar progress bar com % estimado  
**Complexidade**: 3 pontos | **Tempo**: 60 min  
**Arquivo**: `client/src/components/StreamingPromptExecutor.tsx`

**T-27.23** - Adicionar indicador "Tempo estimado: Xs restantes"  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `client/src/components/StreamingPromptExecutor.tsx`

**T-27.24** - Adicionar mensagem de timeout com op√ß√£o retry  
**Complexidade**: 2 pontos | **Tempo**: 30 min  
**Arquivo**: `client/src/components/StreamingPromptExecutor.tsx`

**T-27.25** - Configurar input `maxTokens` e `timeout` (opcional, UI avan√ßada)  
**Complexidade**: 3 pontos | **Tempo**: 45 min  
**Arquivo**: `client/src/components/StreamingPromptExecutor.tsx`

**T-27.26** - Build frontend e valida√ß√£o TypeScript  
**Complexidade**: 1 ponto | **Tempo**: 10 min  

---

#### FASE 5: Testes e Deploy (4 tarefas)

**T-27.27** - Criar suite de testes automatizados completa  
**Complexidade**: 5 pontos | **Tempo**: 90 min  
**Arquivos**: `/tmp/test_sprint_27_*.js`

**T-27.28** - Executar todos os testes (frontend + backend)  
**Complexidade**: 2 pontos | **Tempo**: 20 min  

**T-27.29** - Build produ√ß√£o (client + server)  
**Complexidade**: 2 pontos | **Tempo**: 15 min  

**T-27.30** - Deploy PM2 e valida√ß√£o em produ√ß√£o  
**Complexidade**: 2 pontos | **Tempo**: 15 min  

---

## üìà ESTIMATIVAS

### Complexidade Total: **60 pontos** (Fibonacci)

### Tempo Estimado: **12-14 horas**

**Distribui√ß√£o**:
- FASE 1 (Backend LM Studio): 2.5h
- FASE 2 (Backend REST API): 3.5h
- FASE 3 (Frontend Hook): 2.5h
- FASE 4 (Frontend UI): 3h
- FASE 5 (Testes e Deploy): 2h

### Velocity Esperada: **8-10 tarefas/dia**

---

## üß™ ESTRAT√âGIA DE TESTES (CHECK - PDCA)

### Testes Unit√°rios

**Backend**:
```javascript
// server/lib/__tests__/lm-studio.test.ts
describe('LMStudioClient', () => {
  test('should add max_tokens to request', () => {});
  test('should validate max_tokens range (50-4096)', () => {});
  test('should use default max_tokens=1024', () => {});
});
```

**Frontend**:
```typescript
// client/src/hooks/__tests__/useStreamingPrompt.test.ts
describe('useStreamingPrompt', () => {
  test('should timeout after specified duration', () => {});
  test('should handle SSE timeout event', () => {});
  test('should calculate ETA correctly', () => {});
});
```

### Testes End-to-End

**Teste 1**: Streaming com `max_tokens=100` (deve completar rapidamente)
```bash
Expected: < 10 segundos
Expected chunks: < 100
Expected: Evento DONE recebido
```

**Teste 2**: Streaming com `max_tokens=1024` (resposta m√©dia)
```bash
Expected: < 30 segundos
Expected chunks: < 1024
Expected: Evento DONE recebido
```

**Teste 3**: Streaming com `timeout=15000` (15s)
```bash
Expected: Timeout se resposta > 15s
Expected: Evento SSE 'timeout' recebido
Expected: Mensagem clara no frontend
```

---

## üìä M√âTRICAS DE SUCESSO (CHECK - PDCA)

### Quantitativas

| M√©trica | Antes (Sprint 26) | Meta (Sprint 27) |
|---------|-------------------|------------------|
| Taxa de sucesso SSE | 71% (5/7 checks) | **100% (7/7 checks)** |
| Tempo m√©dio resposta | 55s (1999 chunks) | **< 15s (< 1024 chunks)** |
| Timeouts em produ√ß√£o | Desconhecido | **0 timeouts** |
| Testes aprovados | 6/9 (67%) | **9/9 (100%)** |

### Qualitativas

- ‚úÖ Usu√°rio recebe feedback claro de progresso
- ‚úÖ Usu√°rio pode configurar timeout (UI avan√ßada)
- ‚úÖ Sistema previne respostas excessivamente longas
- ‚úÖ C√≥digo bem documentado e testado

---

## üöÄ RISCOS E MITIGA√á√ïES

### Risco #1: `max_tokens` muito baixo trunca respostas importantes
**Probabilidade**: M√âDIA  
**Impacto**: ALTO  
**Mitiga√ß√£o**: Default 1024 tokens (~750 palavras), configur√°vel por prompt

### Risco #2: Timeout muito curto frustra usu√°rio
**Probabilidade**: M√âDIA  
**Impacto**: M√âDIO  
**Mitiga√ß√£o**: Default 120s (2 minutos), mensagem clara com op√ß√£o retry

### Risco #3: LM Studio n√£o respeita `max_tokens`
**Probabilidade**: BAIXA  
**Impacto**: ALTO  
**Mitiga√ß√£o**: Adicionar timeout client-side como fallback

---

## üìö DOCUMENTA√á√ÉO NECESS√ÅRIA (ACT - PDCA)

1. **README_SPRINT_27.md** - Overview da solu√ß√£o
2. **API_STREAMING_V2.md** - Documenta√ß√£o atualizada da API SSE
3. **FRONTEND_STREAMING_GUIDE.md** - Guia de uso do hook
4. **SPRINT_27_FINAL_REPORT.md** - Relat√≥rio PDCA completo

---

## ‚úÖ DEFINI√á√ÉO DE PRONTO (DoD)

- [ ] Todos os 30 tarefas completadas
- [ ] Build frontend sem erros TypeScript
- [ ] Build backend sem erros TypeScript
- [ ] Testes unit√°rios aprovados (100%)
- [ ] Testes E2E aprovados (3/3 cen√°rios)
- [ ] Deploy PM2 bem-sucedido
- [ ] Documenta√ß√£o completa
- [ ] Code review aprovado
- [ ] Commit e push para GitHub
- [ ] Pull Request criada e aprovada

---

**Planejamento Criado**: 15 de novembro de 2025, 00:15 -03:00  
**Respons√°vel**: AI Developer (Automated Execution)  
**Aprovador**: Product Owner (Usu√°rio Final)  
**Pr√≥ximo Passo**: Executar Sprint 27 (12-14 horas)
